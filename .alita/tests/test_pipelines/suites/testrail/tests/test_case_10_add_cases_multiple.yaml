name: "TR10 add cases multiple"
priority: High
description: |
  Verify the add_cases tool handles edge case with minimal case data.
  
  Objective: Test bulk creation with minimal parameters.
  
  Expected Behavior:
  - Tool executes successfully with minimal required fields
  - Creates test cases with defaults
  - Returns confirmation of creations

toolkits:
  - id: ${TESTRAIL_TOOLKIT_ID}
    name: ${TESTRAIL_TOOLKIT_NAME}

state:
  add_test_cases_data:
    type: str
    value: '[{"section_id": ${TESTRAIL_SECTION_ID}, "title": "Minimal Test Case - ${TIMESTAMP}", "case_properties": {}}]'
  tool_result:
    type: dict
  test_results:
    type: dict
  created_case_ids:
    type: str

entry_point: invoke_add_cases

nodes:
  - id: invoke_add_cases
    type: toolkit
    tool: add_cases
    toolkit_name: ${TESTRAIL_TOOLKIT_NAME}
    input:
      - add_test_cases_data
    input_mapping:
      add_test_cases_data:
        type: variable
        value: add_test_cases_data
    output:
      - tool_result
    structured_output: true
    transition: extract_case_ids

  - id: extract_case_ids
    type: llm
    model: gpt-4o-2024-11-20
    input:
      - tool_result
    input_mapping:
      system:
        type: fixed
        value: "You are a JSON data extractor."
      task:
        type: fstring
        value: |
          Extract the test case ID from the tool result.
          
          Tool Result: {tool_result}
          
          Find and extract the case ID (it may be in fields like 'id', 'case_id', or within the result text).
          Return ONLY the numeric case ID as a plain string with no additional text or formatting.
          
          Example: If the result contains "Test case #12345", return: 12345
      chat_history:
        type: fixed
        value: []
    output:
      - created_case_ids
    structured_output: true
    transition: cleanup_delete_case

  - id: cleanup_delete_case
    type: toolkit
    tool: delete_case
    toolkit_name: ${TESTRAIL_TOOLKIT_NAME}
    input:
      - created_case_ids
    input_mapping:
      case_id:
        type: variable
        value: created_case_ids
      soft_delete:
        type: fixed
        value: true
    output:
      - cleanup_result
    structured_output: false
    continue_on_error: true
    transition: validate_result

  - id: validate_result
    type: llm
    model: gpt-4o-2024-11-20
    input:
      - tool_result
      - cleanup_result
    input_mapping:
      system:
        type: fixed
        value: "You are a quality assurance validator."
      task:
        type: fstring
        value: |
          Analyze the complete test workflow results.

          Creation Result: {tool_result}
          Cleanup Result: {cleanup_result}

          Expected behavior:
          - Tool executed successfully with minimal case data
          - Created test case with default properties
          - Result confirms case creation
          - Cleanup deleted the created test case
          - No errors or exceptions in creation

          Evaluate:
          1. Did the add_cases tool execute successfully?
          2. Are there any errors in the creation result?
          3. Does the output confirm case creation?
          4. Was cleanup successful (can ignore cleanup errors)?

          Return a JSON object with:
          {{
            "test_passed": true/false,
            "summary": "brief description of outcome",
            "error": "error details if failed, null if passed"
          }}

          Note: test_passed should be based ONLY on creation success, not cleanup.
          Return **ONLY** the JSON object. No markdown formatting, no additional text.
      chat_history:
        type: fixed
        value: []
    output:
      - test_results
    structured_output_dict:
      test_results: "dict"
    transition: END
