name: "QT14 - find_test_case_by_id: Handle non-existent test case (Negative Test)"
description: |
  Verify the QTest 'find_test_case_by_id' tool properly handles requests for non-existent test cases.

  Objective: Test error handling when querying a test case ID that doesn't exist

  Expected Behavior (NEGATIVE TEST):
  - Tool returns error for non-existent test case
  - Error message mentions test case not found
  - Proper error handling without crashing
  - No test case data is returned

toolkits:
  - id: ${QTEST_TOOLKIT_ID}
    name: ${QTEST_TOOLKIT_NAME}

state:
  project_id:
    type: int
    value: ${QTEST_PROJECT_ID}
  nonexistent_test_id:
    type: str
    value: "TC-999999999"
  find_result:
    type: str
  expected_error_indicators:
    type: list
    value: ["not found", "does not exist", "error", "failed", "Unable to find"]
  unexpected_success_indicators:
    type: list
    value: ["QTest Id", "Name", "Description", "Steps"]
  test_results:
    type: dict

entry_point: find_nonexistent_test_case
nodes:
  - id: find_nonexistent_test_case
    type: toolkit
    input:
      - nonexistent_test_id
    input_mapping:
      test_id:
        type: variable
        value: nonexistent_test_id
    output:
      - find_result
    structured_output: false
    tool: find_test_case_by_id
    toolkit_name: ${QTEST_TOOLKIT_NAME}
    transition: validate_error_handling

  - id: validate_error_handling
    type: llm
    input:
      - find_result
      - nonexistent_test_id
      - expected_error_indicators
      - unexpected_success_indicators
    input_mapping:
      chat_history:
        type: fixed
        value: []
      system:
        type: fixed
        value: You are a strict quality assurance validator. Only return valid JSON.
      task:
        type: fstring
        value: |
          Analyze the output from the 'find_test_case_by_id' tool execution.
          
          This is a NEGATIVE test - we EXPECT an error for non-existent test case.

          Find Result: {find_result}
          Non-existent Test ID Used: {nonexistent_test_id}
          Expected Error Indicators: {expected_error_indicators}
          Unexpected Success Indicators: {unexpected_success_indicators}

          Validation Criteria (ALL must pass for test_passed=true):
          1. tool_executed: Tool was called (even if it returned an error)
          2. returned_error: Output contains error message
          3. mentions_not_found: Error message indicates test case was not found
          4. no_test_data: Output does NOT contain test case data structures
          5. no_success_indicators: Output does NOT contain success indicators

          Return JSON test_results:
          {{
            "test_passed": boolean (true only if ALL criteria pass - proper error handling),
            "tool_executed": boolean,
            "returned_error": boolean,
            "mentions_not_found": boolean,
            "no_test_data": boolean,
            "no_success_indicators": boolean,
            "error_indicator_found": "which error indicator was found or 'none'",
            "unexpected_success_found": "which success indicator was found or 'none' (should be 'none')",
            "error": string | null
          }}

          Return ONLY the JSON object.
    model: ${DEFAULT_LLM_MODEL}
    output:
      - test_results
    structured_output_dict:
      test_results: "dict"
    transition: end

  - id: end
    type: end
