name: "XR09 - get_test_step_attachments: Handle non-existent test (Negative Test)"
description: |
  Verify the Xray 'get_test_step_attachments' tool properly handles requests for non-existent tests.

  Objective: Test error handling when querying attachments for a test that doesn't exist

  Expected Behavior (NEGATIVE TEST):
  - Tool returns error for non-existent test ID
  - Error message mentions test not found
  - Proper error handling without crashing
  - No attachment data is returned

toolkits:
  - id: ${XRAY_TOOLKIT_ID}
    name: ${XRAY_TOOLKIT_NAME}

state:
  project_key:
    type: str
    value: ${XRAY_PROJECT_KEY:EL}
  nonexistent_issue_id:
    type: str
    value: "999999999"
  get_attachments_result:
    type: str
  expected_error_indicators:
    type: list
    value: ["Tool execution error", "Test not found", "not found", "Failed to get", "does not exist", "doesn't exist"]
  unexpected_success_indicators:
    type: list
    value: ["total_attachments:", "downloadLink", "attachment_id", "filename"]
  test_results:
    type: dict

entry_point: get_attachments_for_nonexistent_test
nodes:
  - id: get_attachments_for_nonexistent_test
    type: toolkit
    input:
      - nonexistent_issue_id
    input_mapping:
      issue_id:
        type: variable
        value: nonexistent_issue_id
    output:
      - get_attachments_result
    structured_output: false
    tool: get_test_step_attachments
    toolkit_name: ${XRAY_TOOLKIT_NAME}
    continue_on_error: true
    transition: validate_error_handling

  - id: validate_error_handling
    type: llm
    input:
      - get_attachments_result
      - nonexistent_issue_id
      - expected_error_indicators
      - unexpected_success_indicators
    input_mapping:
      chat_history:
        type: fixed
        value: []
      system:
        type: fixed
        value: You are a strict quality assurance validator. Only return valid JSON.
      task:
        type: fstring
        value: |
          Analyze the result from executing 'get_test_step_attachments' with a non-existent test ID.
          
          CRITICAL REQUIREMENT:
          This is a NEGATIVE test - we expect a proper error message, NOT empty/null output.
          Empty or null error messages indicate BROKEN error handling and should FAIL the test.
          
          Get Attachments Result: {get_attachments_result}
          Non-existent Issue ID Used: {nonexistent_issue_id}
          Expected Error Indicators: {expected_error_indicators}
          Unexpected Success Indicators: {unexpected_success_indicators}

          VALIDATION LOGIC:
          This negative test PASSES only if the tool returns a proper error message.

          Validation Criteria (ALL must pass for test_passed=true):
          1. tool_executed: Always true (we reached this validation node)
          2. has_error_message: Result is NOT empty/null/None (length > 10 characters)
          3. error_message_meaningful: Result contains at least one expected error indicator
          4. no_attachment_data: Result does NOT contain words "attachments", "downloadLink", "filename", "attachment_id"
          5. no_success_indicators: Result does NOT contain any success field indicators from the list

          FAILURE CONDITIONS:
          - If result is empty, null, or < 10 characters: has_error_message = false, TEST FAILS
          - If result does not contain any expected error indicators: error_message_meaningful = false, TEST FAILS
          - If result contains success indicators: TEST FAILS

          Return JSON test_results:
          {{
            "test_passed": boolean (true only if ALL criteria pass),
            "tool_executed": true,
            "has_error_message": boolean (true if result is non-empty with length > 10),
            "error_message_meaningful": boolean (true if contains expected error indicators),
            "no_attachment_data": boolean (true if no attachment-related words found),
            "no_success_indicators": boolean (true if no success indicators found),
            "result_length": number (character count of result),
            "error_indicator_found": "which error indicator was found or 'none'",
            "result_preview": "first 150 chars of result or 'EMPTY/NULL - ERROR HANDLING BROKEN'" 
          }}

          Return ONLY the JSON object.
    model: ${DEFAULT_LLM_MODEL}
    output:
      - test_results
    transition: END
