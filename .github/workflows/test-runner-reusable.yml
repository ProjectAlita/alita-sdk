name: Test Runner Reusable
run-name: Executing test for ${{ inputs.test_cases_dir }} on ${{ inputs.environment }} 

on:
  workflow_call:
    inputs:
      test_cases_dir:
        description: 'Test cases directory (e.g., github, ado, confluence)'
        required: true
        type: string
      environment:
        description: 'Environment to run tests in (local or other)'
        required: false
        type: string
        default: 'local'
      enable_parallel:
        description: 'Enable parallel test execution (uses value from pipeline.yaml when enabled)'
        required: false
        type: string
        default: 'false'
      enable_bug_reporting:
        description: 'Enable automatic bug report creation on GitHub (disable for testing)'
        required: false
        type: string
        default: 'true'

env:
  REGISTRY: ghcr.io
  IMAGE_NAME: ${{ github.repository }}

jobs:
  test-execution:
    runs-on: ubuntu-latest
    permissions:
      contents: read
      packages: read
      pull-requests: write

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Install Docker Compose
        run: |
          sudo curl -L "https://github.com/docker/compose/releases/latest/download/docker-compose-$(uname -s)-$(uname -m)" -o /usr/local/bin/docker-compose
          sudo chmod +x /usr/local/bin/docker-compose
          docker-compose --version

      - name: Log in to GitHub Container Registry
        uses: docker/login-action@v3
        with:
          registry: ${{ env.REGISTRY }}
          username: ${{ github.actor }}
          password: ${{ secrets.GITHUB_TOKEN }}

      - name: Pull Docker image
        run: |
          IMAGE_NAME=$(echo "${{ github.repository }}" | tr '[:upper:]' '[:lower:]')
          docker pull ${{ env.REGISTRY }}/$IMAGE_NAME:pyodide

      - name: Calculate and run test command
        run: |
          ENV_FLAG=$([ "${{ inputs.environment }}" = "local" ] && echo "--local" || echo "")
          PARALLEL_FLAG=$([ "${{ inputs.enable_parallel }}" = "true" ] && echo "--parallel" || echo "--no-parallel")
          TEST_COMMAND="/app/.alita/tests/test_pipelines/run_all_suites.sh $ENV_FLAG $PARALLEL_FLAG -v suites/${{ inputs.test_cases_dir }}"
          
          echo "DOCKER_COMMAND_COMPOSED=$TEST_COMMAND" >> $GITHUB_ENV  
          echo "Running test command: $TEST_COMMAND"         

      - name: Uppercase the environment
        id: set-api-key
        run: |
          if [ "${{ inputs.environment }}" = "local" ]; then
            ENV_UPPER="DEV"
          else
            ENV_UPPER=$(echo "${{ inputs.environment }}" | tr '[:lower:]' '[:upper:]')
          fi
          echo "ENV_UPPER=${ENV_UPPER}" >> $GITHUB_ENV

      - name: Run docker-compose
        run: |
          echo "::notice::Starting docker-compose with command: ${{ env.DOCKER_COMMAND_COMPOSED }}"
          echo "::notice::On environment: ${{ env.ENV_UPPER }}"
          export DEPLOYMENT_URL="${{ secrets[format('DEPLOYMENT_URL_{0}', env.ENV_UPPER )] }}"
          export ALITA_API_KEY="${{ secrets[format('ALITA_API_KEY_{0}', env.ENV_UPPER )] }}"
          export PROJECT_ID="${{ secrets[format('PROJECT_ID_{0}', env.ENV_UPPER )] }}"
          export GIT_TOOL_ACCESS_TOKEN="${{ secrets.GIT_TOOL_ACCESS_TOKEN }}"
          export CONFLUENCE_API_KEY="${{ secrets.CONFLUENCE_API_KEY }}"
          export CONFLUENCE_BASE_URL="${{ secrets.CONFLUENCE_BASE_URL }}"
          export CONFLUENCE_USERNAME="${{ secrets.CONFLUENCE_USERNAME }}"
          export ADO_TOKEN="${{ secrets.ADO_TOKEN }}"
          export ADO_REPOSITORY_ID="${{ secrets.ADO_REPOSITORY_ID }}"
          export ADO_ORGANIZATION_URL="${{ secrets.ADO_ORGANIZATION_URL }}"
          export ADO_PROJECT="${{ secrets.ADO_PROJECT }}"
          export JIRA_API_KEY="${{ secrets.JIRA_API_KEY }}"
          export JIRA_USERNAME="${{ secrets.JIRA_USERNAME }}"
          export JIRA_BASE_URL="${{ secrets.JIRA_BASE_URL }}"
          export FIGMA_API_TOKEN="${{ secrets.FIGMA_API_TOKEN }}"
          export BITBUCKET_TOKEN="${{ secrets.BITBUCKET_TOKEN }}"
          export XRAY_CLIENT_SECRET="${{ secrets.XRAY_CLIENT_SECRET }}"
          export XRAY_CLIENT_ID="${{ secrets.XRAY_CLIENT_ID }}"
          export ZEPHYR_ESENTIALS_KEY="${{ secrets.ZEPHYR_ESENTIALS_KEY }}"
          export GITLAB_PRIVATE_TOKEN="${{ secrets.GITLAB_PRIVATE_TOKEN }}"
          export POSTMAN_API_KEY="${{ secrets.POSTMAN_API_KEY }}"
          export SHAREPOINT_CLIENT_SECRET="${{ secrets.SHAREPOINT_CLIENT_SECRET }}"
          export JIRA_PROJECT="EL"
          export TEST_CASES_DIR="${{ inputs.test_cases_dir}}"
          export ENVIRONMENT="${{ inputs.environment }}"
          export DOCKER_COMMAND="${{ env.DOCKER_COMMAND_COMPOSED }}"
          export ENV_NAME="${{ inputs.environment }}"
          docker-compose up -d

      - name: Wait for tests to complete
        id: wait_tests
        run: |
          echo "Waiting for test execution to complete..."
          
          TIMEOUT=10000
          ELAPSED=0
          INTERVAL=20
          CONTAINER_EXIT_CODE=0
          
          while [ $ELAPSED -lt $TIMEOUT ]; do
            STATUS=$(docker inspect alita-sdk-dev --format='{{.State.Status}}' 2>/dev/null || echo "not-found")
            
            if [ "$STATUS" = "exited" ]; then
              EXIT_CODE=$(docker inspect alita-sdk-dev --format='{{.State.ExitCode}}')
              CONTAINER_EXIT_CODE=$EXIT_CODE
              echo "Container exited with code: $EXIT_CODE"
              
              if [ "$EXIT_CODE" -eq 0 ]; then
                echo "‚úÖ Container completed successfully"
                echo "tests_failed=false" >> $GITHUB_OUTPUT
              else
                echo "‚ùå Container exited with error code $EXIT_CODE"
                echo "tests_failed=true" >> $GITHUB_OUTPUT
              fi
              
              echo "Container logs:"
              docker-compose logs alita-sdk
              break
            elif [ "$STATUS" = "running" ]; then
              echo "Container still running... ($ELAPSED seconds elapsed)"
              echo "::group:: Current logs"
              docker-compose logs alita-sdk --tail=50
              echo "::endgroup::"
            else
              echo "Container status: $STATUS"
            fi
            
            sleep $INTERVAL
            ELAPSED=$((ELAPSED + INTERVAL))
          done
          
          if [ $ELAPSED -ge $TIMEOUT ]; then
            echo "‚ö†Ô∏è Timeout reached after $TIMEOUT seconds"
            echo "tests_failed=true" >> $GITHUB_OUTPUT
            docker-compose logs alita-sdk
            exit 1
          fi
          
          if [ $CONTAINER_EXIT_CODE -ne 0 ]; then
            echo "::error::Tests failed with exit code $CONTAINER_EXIT_CODE"
            exit $CONTAINER_EXIT_CODE
          fi

      - name: Copy test results
        if: always()
        run: |
          docker cp alita-sdk-dev:/app/.alita/tests/test_pipelines/test_results/suites/${{ inputs.test_cases_dir }}/results.json ./results.json || echo "{}" > ./results.json
          docker cp alita-sdk-dev:/app/.alita/tests/test_pipelines/test_results/suites/${{ inputs.test_cases_dir }}/results_for_bug_reporter.json ./results_for_bug_reporter.json || echo "{}" > ./results_for_bug_reporter.json
          
      - name: Generate test report
        if: always()
        run: |
          echo "Generating test report..."
          python .alita/tests/test_pipelines/scripts/generate_report.py ./results.json

      - name: Print execution summary
        if: always()
        id: test_summary
        run: |
          if [ ! -f ./results.json ]; then
            echo "‚ùå No results.json found - tests may not have completed"
            if [ -n "$GITHUB_STEP_SUMMARY" ]; then
              echo "" >> "$GITHUB_STEP_SUMMARY"
              echo "## ‚ùå Test Results" >> "$GITHUB_STEP_SUMMARY"
              echo "" >> "$GITHUB_STEP_SUMMARY"
              echo "‚ùå No results.json found - tests may not have completed" >> "$GITHUB_STEP_SUMMARY"
            fi
            exit 1
          fi
          
          # Parse JSON with jq
          SUITE=$(jq -r '.suite_name // "unknown"' ./results.json)
          PASSED=$(jq -r '.passed // 0' ./results.json)
          FAILED=$(jq -r '.failed // 0' ./results.json)
          ERRORS=$(jq -r '.errors // 0' ./results.json)
          SKIPPED=$(jq -r '.skipped // 0' ./results.json)
          
          # Determine status based on failures, errors, and container exit code
          TESTS_FAILED="${{ steps.wait_tests.outputs.tests_failed }}"
          if [ "$FAILED" -gt 0 ] || [ "$ERRORS" -gt 0 ] || [ "$TESTS_FAILED" = "true" ]; then
            STATUS_DISPLAY="‚ùå FAILED"
            ALERT_TYPE="CAUTION"
            ALERT_MSG="Tests failed for suite **${SUITE}**"
          else
            STATUS_DISPLAY="‚úÖ PASSED"
            ALERT_TYPE="TIP"
            ALERT_MSG="All tests passed for suite **${SUITE}**"
          fi
                    
          # Write to GitHub Step Summary
          if [ -n "$GITHUB_STEP_SUMMARY" ]; then
            {
              echo ""
              echo "## üìä Test Execution Summary"
              echo ""
              echo "> [!${ALERT_TYPE}]"
              echo "> ${ALERT_MSG}"
              echo ""
              echo "| Suite | Status | Passed | Failed | Errors | Skipped |"
              echo "|-------|--------|--------|--------|--------|--------|"
              echo "| ${SUITE} | ${STATUS_DISPLAY} | ${PASSED} | ${FAILED} | ${ERRORS} | ${SKIPPED} |"
              echo ""
            } >> "$GITHUB_STEP_SUMMARY"
          fi

      - name: Save Docker logs
        if: always()
        run: |
          docker-compose logs alita-sdk > docker-logs.txt 2>&1 || echo "Failed to retrieve logs" > docker-logs.txt
      
      - name: Do AI analysis of logs
        if: ${{ always() && steps.wait_tests.outputs.tests_failed == 'true' }}
        run: |
          echo "::notice::Running AI analysis of test results"
          
          # Re-export all environment variables
          export DEPLOYMENT_URL="${{ secrets[format('DEPLOYMENT_URL_{0}', env.ENV_UPPER )] }}"
          export ALITA_API_KEY="${{ secrets[format('ALITA_API_KEY_{0}', env.ENV_UPPER )] }}"
          export PROJECT_ID="${{ secrets[format('PROJECT_ID_{0}', env.ENV_UPPER )] }}"
          export GIT_TOOL_ACCESS_TOKEN="${{ secrets.GIT_TOOL_ACCESS_TOKEN }}"
          export CONFLUENCE_API_KEY="${{ secrets.CONFLUENCE_API_KEY }}"
          export CONFLUENCE_BASE_URL="${{ secrets.CONFLUENCE_BASE_URL }}"
          export CONFLUENCE_USERNAME="${{ secrets.CONFLUENCE_USERNAME }}"
          export ADO_TOKEN="${{ secrets.ADO_TOKEN }}"
          export ADO_REPOSITORY_ID="${{ secrets.ADO_REPOSITORY_ID }}"
          export ADO_ORGANIZATION_URL="${{ secrets.ADO_ORGANIZATION_URL }}"
          export ADO_PROJECT="${{ secrets.ADO_PROJECT }}"
          export JIRA_API_KEY="${{ secrets.JIRA_API_KEY }}"
          export JIRA_USERNAME="${{ secrets.JIRA_USERNAME }}"
          export JIRA_BASE_URL="${{ secrets.JIRA_BASE_URL }}"
          export FIGMA_API_TOKEN="${{ secrets.FIGMA_API_TOKEN }}"
          export BITBUCKET_TOKEN="${{ secrets.BITBUCKET_TOKEN }}"
          export XRAY_CLIENT_SECRET="${{ secrets.XRAY_CLIENT_SECRET }}"
          export XRAY_CLIENT_ID="${{ secrets.XRAY_CLIENT_ID }}"
          export ZEPHYR_ESENTIALS_KEY="${{ secrets.ZEPHYR_ESENTIALS_KEY }}"
          export GITLAB_PRIVATE_TOKEN="${{ secrets.GITLAB_PRIVATE_TOKEN }}"
          export JIRA_PROJECT="EL"
          export TEST_CASES_DIR="${{ inputs.test_cases_dir}}"
          export ENVIRONMENT="${{ inputs.environment }}"
          export ENV_NAME="${{ inputs.environment }}"
          
          # Build the results path first
          RESULTS_PATH=".alita/tests/test_pipelines/test_results/suites/${{ inputs.test_cases_dir }}/results.json"
          
          format_branch() {
              echo "${1#refs/heads/}"
          }
          
          formated_branch=$(format_branch "${{ github.head_ref || github.ref }}")

          # Run without --rm to keep container for file extraction
          docker-compose run --name alita-sdk-fixer alita-sdk \
          sh -c "git config --global --add safe.directory /app && alita agent run .alita/agents/test-fixer.agent.md --dir . 'Analyze ${RESULTS_PATH} on ${{ inputs.environment }} and branch ${formated_branch}'" || echo "AI analysis completed with warnings"

      - name: Copy AI analysis outputs
        if: ${{ always() && steps.wait_tests.outputs.tests_failed == 'true' }}
        run: |
          echo "Copying fix analysis files from container..."
          docker cp alita-sdk-fixer:/app/.alita/tests/test_pipelines/test_results/suites/${{ inputs.test_cases_dir }}/fix_output.json ./fix_output.json || echo "{}" > ./fix_output.json
          docker cp alita-sdk-fixer:/app/.alita/tests/test_pipelines/test_results/suites/${{ inputs.test_cases_dir }}/fix_milestone.json ./fix_milestone.json || echo "{}" > ./fix_milestone.json
          
          echo "fix_output.json content:"
          cat ./fix_output.json
          
          # Remove the fixer container after copying files
          docker rm alita-sdk-fixer || true

      - name: Label PR with tests_ai_healed
        if: ${{ always() && steps.wait_tests.outputs.tests_failed == 'true' }}
        run: |
          if [ ! -f ./fix_output.json ]; then
            echo "No fix_output.json found, skipping PR labeling"
            exit 0
          fi
          
          # Check if fixes were committed
          COMMITTED=$(jq -r '.committed // false' ./fix_output.json)
          if [ "$COMMITTED" != "true" ]; then
            echo "No fixes committed, skipping PR labeling"
            exit 0
          fi
          
          # Get PR number from fix_output.json
          PR_NUMBER=$(jq -r '.commit_details.pr_number // null' ./fix_output.json)
          if [ "$PR_NUMBER" = "null" ]; then
            echo "No PR found for branch, skipping labeling"
            exit 0
          fi
          
          echo "Adding 'tests_ai_healed' label to PR #$PR_NUMBER"
          
          # Add label to PR
          gh pr edit "$PR_NUMBER" \
            --add-label tests_ai_healed \
            --repo ${{ github.repository }} || echo "Failed to add label, continuing..."
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}

      - name: Post AI Analysis Summary to GitHub
        if: ${{ always() && steps.wait_tests.outputs.tests_failed == 'true' }}
        run: |
          if [ ! -f ./fix_output.json ]; then
            echo "::warning::No AI analysis output found"
            exit 0
          fi
          
          # Check if file is valid JSON
          if ! jq empty ./fix_output.json 2>/dev/null; then
            echo "::warning::Invalid JSON in fix_output.json"
            exit 0
          fi
          
          # Parse AI analysis results
          FIXED_COUNT=$(jq -r '.summary.fixed // 0' ./fix_output.json)
          FLAKY_COUNT=$(jq -r '.summary.flaky // 0' ./fix_output.json)
          BLOCKED_COUNT=$(jq -r '.summary.blocked // 0' ./fix_output.json)
          TOTAL_ANALYZED=$(jq -r '(.summary.total_analyzed // ((.summary.fixed // 0) + (.summary.flaky // 0) + (.summary.blocked // 0)))' ./fix_output.json)
          
          # Count SDK bugs from intent analysis in milestone
          SDK_BUG_COUNT=0
          TEST_CODE_ISSUE_COUNT=0
          INTENT_ANALYSIS_COUNT=0
          if [ -f ./fix_milestone.json ] && jq empty ./fix_milestone.json 2>/dev/null; then
            SDK_BUG_COUNT=$(jq -r '[.intent_analysis[]? | select(.classification == "sdk_bug")] | length' ./fix_milestone.json)
            TEST_CODE_ISSUE_COUNT=$(jq -r '[.intent_analysis[]? | select(.classification == "test_code_issue")] | length' ./fix_milestone.json)
            INTENT_ANALYSIS_COUNT=$(jq -r '.intent_analysis // [] | length' ./fix_milestone.json)
          fi
          
          # Write AI Analysis Summary to GitHub Step Summary
          if [ -n "$GITHUB_STEP_SUMMARY" ]; then
            {
              echo ""
              echo "## ü§ñ AI Analysis Summary"
              echo ""
              # Show SDK bug warning banner if any SDK bugs detected
              if [ "$SDK_BUG_COUNT" -gt 0 ]; then
                echo "> [!CAUTION]"
                echo "> **${SDK_BUG_COUNT} SDK bug(s) detected** ‚Äî these are real product defects, not test issues. Tests were correctly blocked (not patched to hide bugs)."
                echo ""
              fi
              
              echo "| Metric | Count |"
              echo "|--------|-------|"
              echo "| Total Analyzed | **${TOTAL_ANALYZED}** |"
              echo "| ‚úÖ Fixed (test code issues) | ${FIXED_COUNT} |"
              echo "| ‚ö†Ô∏è Flaky | ${FLAKY_COUNT} |"
              echo "| ‚ùå Blocked (SDK bugs) | ${BLOCKED_COUNT} |"
              if [ "$INTENT_ANALYSIS_COUNT" -gt 0 ]; then
                echo "| üîç SDK Bugs Identified | ${SDK_BUG_COUNT} |"
                echo "| üîß Test Code Issues | ${TEST_CODE_ISSUE_COUNT} |"
              fi
              echo ""
              
              # Show fixed tests details
              if [ "$FIXED_COUNT" -gt 0 ]; then
                echo "### ‚úÖ Fixed Tests"
                echo ""
                echo "| Test | Root Cause | Fix Applied |"
                echo "|------|------------|-------------|"
                jq -r '.fixed[]? | "| \((.test_ids // []) | join(", ")) | \(.issue // "N/A") | \(.fix // "N/A") |"' ./fix_output.json
                echo ""
              fi
              
              # Show flaky tests details
              if [ "$FLAKY_COUNT" -gt 0 ]; then
                echo "### ‚ö†Ô∏è Flaky Tests"
                echo ""
                echo "> [!WARNING]"
                echo "> The following tests failed initially but passed on rerun"
                echo ""
                echo "| Test | Reason |"
                echo "|------|--------|"
                jq -r '.flaky[]? | "| \((.test_ids // []) | join(", ")) | \(.reason // "N/A") |"' ./fix_output.json
                echo ""
              fi
              
              # Show blocked tests details
              if [ "$BLOCKED_COUNT" -gt 0 ]; then
                echo "### ‚ùå Blocked Tests (SDK Bugs)"
                echo ""
                echo "> [!IMPORTANT]"
                echo "> These tests were **not patched** because the failures are caused by SDK code defects, not test logic errors. Weakening assertions would hide real bugs."
                echo ""
                jq -r '.blocked[]? | "<details>\n<summary><b>\((.test_ids // []) | join(", "))</b> ‚Äî \(.bug_description // "N/A")</summary>\n\n| Field | Detail |\n|-------|--------|\n| Bug Description | \(.bug_description // "N/A") |\n| Expected Behavior | \(.expected_behavior // "N/A") |\n| Actual Behavior | \(.actual_behavior // "N/A") |\n| SDK Component | \(.sdk_component // "N/A") |\n| Affected Methods | \((.affected_methods // []) | join(", ")) |\n| Error Location | \(.error_location // "N/A") |\n| Bug Report Needed | \(.bug_report_needed // false) |\n\n</details>\n"' ./fix_output.json
                echo ""
              fi
              
              # Show Intent Analysis from milestone (how each test was classified)
              if [ -f ./fix_milestone.json ] && jq empty ./fix_milestone.json 2>/dev/null; then
                INTENT_COUNT=$(jq -r '.intent_analysis // [] | length' ./fix_milestone.json)
                if [ "$INTENT_COUNT" -gt 0 ]; then
                  echo "<details>"
                  echo "<summary>üîç <b>Test Intent Analysis</b> ‚Äî ${INTENT_COUNT} tests classified</summary>"
                  echo ""
                  echo "> How the AI determined whether each failure is an SDK bug or a test code issue"
                  echo ""
                  echo "| Test | Type | Classification | Primary Feature | SDK Correct? | Reasoning |"
                  echo "|------|------|----------------|-----------------|--------------|-----------|" 
                  jq -r '.intent_analysis[]? | "| \(.test_id // "N/A") | \(.positive_or_negative // "N/A") | \(if .classification == "sdk_bug" then "üêõ SDK Bug" else "üîß Test Issue" end) | \(.primary_feature // "N/A") | \(if .sdk_behaves_correctly then "‚úÖ" else "‚ùå" end) | \(.reasoning // "N/A") |"' ./fix_milestone.json
                  echo ""
                  echo "</details>"
                  echo ""
                fi
              fi
              
              # Add milestone summary if available (collapsible)
              if [ -f ./fix_milestone.json ]; then
                if jq empty ./fix_milestone.json 2>/dev/null; then
                  # Gather milestone data
                  MS_ENV=$(jq -r '.environment // "N/A"' ./fix_milestone.json)
                  MS_BRANCH=$(jq -r '.ci_target_branch // "N/A"' ./fix_milestone.json)
                  MS_SUITE=$(jq -r '.suite // "N/A"' ./fix_milestone.json)
                  MS_FAILED_COUNT=$(jq -r '.initial_failures // [] | length' ./fix_milestone.json)
                  FIX_ATTEMPTS_COUNT=$(jq -r '.fix_attempts // [] | length' ./fix_milestone.json)
                  RERUN_COUNT=$(jq -r '.rerun_attempts // [] | length' ./fix_milestone.json)
                  
                  echo "<details>"
                  echo "<summary>üìã <b>Fix Milestone</b> ‚Äî ${MS_FAILED_COUNT} failures, ${FIX_ATTEMPTS_COUNT} fix attempts</summary>"
                  echo ""
                  echo "| Field | Value |"
                  echo "|-------|-------|"
                  echo "| Environment | ${MS_ENV} |"
                  echo "| Branch | \`${MS_BRANCH}\` |"
                  echo "| Suite | ${MS_SUITE} |"
                  echo "| Failed Tests | ${MS_FAILED_COUNT} |"
                  echo ""
                  
                  # Show error patterns as table with classification
                  ERROR_PATTERNS_COUNT=$(jq -r '.error_patterns // [] | length' ./fix_milestone.json)
                  if [ "$ERROR_PATTERNS_COUNT" -gt 0 ]; then
                    echo "**Error Patterns:**"
                    echo ""
                    echo "| Pattern | Tests | Category |"
                    echo "|---------|-------|----------|"
                    jq -r '.error_patterns[]? | "| \(.root_cause // "Unknown") | \((.test_ids // []) | join(", ")) | \(if .category == "sdk_bug" then "üêõ SDK Bug" else "üîß Test Issue" end) |"' ./fix_milestone.json
                    echo ""
                  fi
                  
                  # Show fix attempts as table with intent preservation
                  if [ "$FIX_ATTEMPTS_COUNT" -gt 0 ]; then
                    echo "**Fix Attempts:**"
                    echo ""
                    echo "| # | Rationale | Intent Preserved |"
                    echo "|---|-----------|------------------|"
                    jq -r '.fix_attempts[]? | "| \(.attempt) | \(.fix_rationale // "N/A") | \(if .intent_preserved == false then "‚ùå No" elif .intent_preserved == true then "‚úÖ Yes" else "N/A" end) |"' ./fix_milestone.json
                    echo ""
                    
                    # Warn if any fix did not preserve intent
                    INTENT_BROKEN=$(jq -r '[.fix_attempts[]? | select(.intent_preserved == false)] | length' ./fix_milestone.json)
                    if [ "$INTENT_BROKEN" -gt 0 ]; then
                      echo "> [!WARNING]"
                      echo "> **${INTENT_BROKEN} fix attempt(s) did not preserve test intent** ‚Äî these fixes were reverted and tests classified as blockers to avoid hiding SDK bugs."
                      echo ""
                    fi
                  fi
                  
                  # Show rerun attempts
                  if [ "$RERUN_COUNT" -gt 0 ]; then
                    echo "**Rerun Attempts:** ${RERUN_COUNT}"
                    echo ""
                  fi
                  
                  echo "</details>"
                  echo ""
                fi
              fi
              
              # Add commit information if fixes were committed
              COMMITTED=$(jq -r '.committed // false' ./fix_output.json)
              if [ "$COMMITTED" = "true" ]; then
                COMMIT_MSG=$(jq -r '.commit_details.commit_message // null' ./fix_output.json)
                if [ "$COMMIT_MSG" = "null" ] && [ -f ./fix_milestone.json ]; then
                  COMMIT_MSG=$(jq -r '.commit_info.commit_message // "N/A"' ./fix_milestone.json)
                fi
                BRANCH=$(jq -r '.commit_details.branch // "N/A"' ./fix_output.json)
                FILES_COUNT=$(jq -r '.commit_details.files_count // 0' ./fix_output.json)
                PR_NUMBER=$(jq -r '.commit_details.pr_number // null' ./fix_output.json)
                
                echo "> [!TIP]"
                echo "> Fixes committed and pushed to branch \`${BRANCH}\`"
                echo ""
                echo "| Detail | Value |"
                echo "|--------|-------|"
                echo "| Files updated | ${FILES_COUNT} |"
                echo "| Commit message | ${COMMIT_MSG} |"
                
                # Add PR info if found
                if [ "$PR_NUMBER" != "null" ]; then
                  echo "| PR | [#${PR_NUMBER}](https://github.com/ProjectAlita/alita-sdk/pull/${PR_NUMBER}) |"
                  echo "| Label | \`tests_ai_healed\` (auto-applied) |"
                else
                  echo "| PR | _No PR found for branch_ |"
                fi
                echo ""
              fi
            } >> "$GITHUB_STEP_SUMMARY"
          fi
          
      - name: Parse fix_output.json and check for blocked tests
        if: ${{ always() && steps.wait_tests.outputs.tests_failed == 'true' }}
        id: check_blocked
        run: |
          echo "Parsing fix_output.json for blocked tests..."
          
          # Check if file exists and is valid JSON
          if [ ! -f ./fix_output.json ]; then
            echo "fix_output.json not found"
            echo "bug_report_needed=false" >> $GITHUB_OUTPUT
            exit 0
          fi
          
          # Parse blocked count
          BLOCKED_COUNT=$(jq -r '.summary.blocked // 0' ./fix_output.json)
          echo "Blocked tests count: $BLOCKED_COUNT"
          
          # Check if any blocked tests need bug reports
          if [ "$BLOCKED_COUNT" -gt 0 ]; then
            BUG_REPORT_NEEDED=$(jq -r '[.blocked[]? | select(.bug_report_needed == true)] | length > 0' ./fix_output.json)
            
            if [ "$BUG_REPORT_NEEDED" = "true" ]; then
              echo "::notice::Bug report needed for blocked tests"
              echo "bug_report_needed=true" >> $GITHUB_OUTPUT
              
              # Export blocked tests details for bug report
              jq -r '.blocked[]? | select(.bug_report_needed == true)' ./fix_output.json > blocked_tests_details.json
              echo "Blocked tests requiring bug reports:"
              cat blocked_tests_details.json
            else
              echo "bug_report_needed=false" >> $GITHUB_OUTPUT
            fi
          else
            echo "No blocked tests found"
            echo "bug_report_needed=false" >> $GITHUB_OUTPUT
          fi

      - name: Create bug reports for blocked tests
        if: ${{ always() && steps.wait_tests.outputs.tests_failed == 'true' && steps.check_blocked.outputs.bug_report_needed == 'true' && inputs.enable_bug_reporting == 'true' }}
        run: |
          echo "::notice::Creating bug reports for blocked tests"
          
          # Export required environment variables
          export DEPLOYMENT_URL="${{ secrets[format('DEPLOYMENT_URL_{0}', env.ENV_UPPER )] }}"
          export ALITA_API_KEY="${{ secrets[format('ALITA_API_KEY_{0}', env.ENV_UPPER )] }}"
          export PROJECT_ID="${{ secrets[format('PROJECT_ID_{0}', env.ENV_UPPER )] }}"
          export GIT_TOOL_ACCESS_TOKEN="${{ secrets.GIT_TOOL_ACCESS_TOKEN }}"
          
          # Build the results path
          RESULTS_PATH=".alita/tests/test_pipelines/test_results/suites/${{ inputs.test_cases_dir }}"
          
          # Run bug reporter agent (files are accessible via volume mount)
          docker-compose run --name alita-sdk-bug-reporter alita-sdk \
          sh -c "alita agent run .alita/agents/bug-reporter.agent.md --dir . 'Analyze data in ${RESULTS_PATH} on ${{ inputs.environment }} environment'" || echo "Bug reporter completed with warnings"
      
      - name: Copy bug reporter outputs
        if: ${{ always() && steps.wait_tests.outputs.tests_failed == 'true' && steps.check_blocked.outputs.bug_report_needed == 'true' && inputs.enable_bug_reporting == 'true' }}
        run: |
          echo "Copying bug report files from container..."
          docker cp alita-sdk-bug-reporter:/app/.alita/tests/test_pipelines/test_results/suites/${{ inputs.test_cases_dir }}/bug_report_output.json ./bug_report_output.json || echo "{}" > ./bug_report_output.json
          
          echo "bug_report_output.json content:"
          cat ./bug_report_output.json
          
          # Remove the bug reporter container after copying files
          docker rm alita-sdk-bug-reporter || true
      
      - name: Post Bug Report Summary to GitHub
        if: ${{ always() && steps.wait_tests.outputs.tests_failed == 'true' && steps.check_blocked.outputs.bug_report_needed == 'true' && inputs.enable_bug_reporting == 'true' }}
        run: |
          if [ ! -f ./bug_report_output.json ]; then
            echo "::warning::No bug report output found"
            exit 0
          fi
          
          # Check if file is valid JSON
          if ! jq empty ./bug_report_output.json 2>/dev/null; then
            echo "::warning::Invalid JSON in bug_report_output.json"
            exit 0
          fi
          
          # Parse bug report results
          BUGS_CREATED=$(jq -r '.summary.bugs_created // 0' ./bug_report_output.json)
          DUPLICATES_SKIPPED=$(jq -r '.summary.duplicates_skipped // 0' ./bug_report_output.json)
          FAILED=$(jq -r '.summary.failed // 0' ./bug_report_output.json)
          TOTAL=$(jq -r '.summary.total_analyzed // 0' ./bug_report_output.json)
          
          # Write Bug Report Summary to GitHub Step Summary
          if [ -n "$GITHUB_STEP_SUMMARY" ]; then
            {
              echo ""
              echo "## üêõ Bug Reporter Summary"
              echo ""
              echo "| Metric | Count |"
              echo "|--------|-------|"
              echo "| Total Analyzed | **${TOTAL}** |"
              echo "| ‚úÖ Bugs Created | ${BUGS_CREATED} |"
              echo "| ‚è≠Ô∏è Duplicates Skipped | ${DUPLICATES_SKIPPED} |"
              echo "| ‚ùå Failed to Report | ${FAILED} |"
              echo ""
              
              # Show created bugs
              if [ "$BUGS_CREATED" -gt 0 ]; then
                echo "### ‚úÖ Bugs Created on ELITEA Board"
                echo ""
                echo "| Issue | Tests | Component | Labels |"
                echo "|-------|-------|-----------|--------|"
                jq -r '.bugs_created[]? | "| [\(.title)](\(.issue_url // ("https://github.com/ProjectAlita/projectalita.github.io/issues/" + (.issue_number | tostring)))) | \((.test_ids // []) | join(", ")) | \(.affected_component // "N/A") | \((.labels // []) | join(", ")) |"' ./bug_report_output.json
                echo ""
              fi
              
              # Show skipped duplicates (collapsible)
              if [ "$DUPLICATES_SKIPPED" -gt 0 ]; then
                echo "<details>"
                echo "<summary>‚è≠Ô∏è <b>Duplicates Skipped</b> (${DUPLICATES_SKIPPED})</summary>"
                echo ""
                echo "> [!NOTE]"
                echo "> The following test failures matched existing bugs and were not re-reported"
                echo ""
                jq -r '.duplicates_skipped[]? | . as $e | (if $e.existing_issue_titles then [$e.existing_issue_titles, (if $e.existing_issue_urls then $e.existing_issue_urls else ($e.existing_issue_numbers // [] | map("https://github.com/ProjectAlita/projectalita.github.io/issues/" + tostring)) end)] | transpose | map("  - **Similar to:** [\(.[0])](\(.[1]))") elif $e.existing_issue_title then ["  - **Similar to:** [\($e.existing_issue_title)](\($e.existing_issue_url // ("https://github.com/ProjectAlita/projectalita.github.io/issues/" + ($e.existing_issue_number | tostring))))"] else ["  - **Similar to:** N/A"] end) as $links | "- **Tests:** \(($e.test_ids // []) | join(", "))\n" + ($links | join("\n")) + "\n  - **Reason:** \($e.similarity_reason // "N/A")\n"' ./bug_report_output.json
                echo ""
                echo "</details>"
                echo ""
              fi
              
              # Show failures (collapsible)
              if [ "$FAILED" -gt 0 ]; then
                echo "<details>"
                echo "<summary>‚ùå <b>Failed to Report</b> (${FAILED})</summary>"
                echo ""
                echo "> [!CAUTION]"
                echo "> The following test failures could not be automatically reported"
                echo ""
                echo "| Tests | Reason | Action |"
                echo "|-------|--------|--------|"
                jq -r '.failed[]? | "| \((.test_ids // []) | join(", ")) | \(.reason // "N/A") | \(.action_needed // "Manual review required") |"' ./bug_report_output.json
                echo ""
                echo "</details>"
                echo ""
              fi
            } >> "$GITHUB_STEP_SUMMARY"
          fi
          
      - name: Stop docker-compose
        if: always()
        run: docker-compose down --remove-orphans

      - name: Upload test results as artifact
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: test-results-${{ inputs.test_cases_dir }}
          path: |
            ./results.html
            ./results_for_bug_reporter.json
            ./docker-logs.txt
            ./fix_output.json
            ./fix_milestone.json
            ./blocked_tests_details.json
            ./bug_report_output.json
          retention-days: 1

      - name: CLEAN UP WORKSPACE AFTER RUN
        if: always()
        run: |
          sudo rm -rf $GITHUB_WORKSPACE/{,.[!.],..?}*
